{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "for elem in data:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    X.append(tf.resize(elem[0], output_shape=(40,40)))\n",
    "    Y.append(elem[1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Y', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.load(\"X.npy\")\n",
    "Y = np.load(\"Y.npy\")\n",
    "\n",
    "X.resize((166708,40,40,1))\n",
    "\n",
    "labels = np.unique(Y)\n",
    "labels = labels.tolist()\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    Y[i] = labels.index(Y[i])\n",
    "\n",
    "X = X.astype(np.float16)\n",
    "Y = Y.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.load(\"X.npy\")\n",
    "Y = np.load(\"Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d1259c42ec6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskeletonize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestoration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdenoise_bilateral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenoise_bilateral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultichannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_adaptive\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.restoration import denoise_bilateral\n",
    "plt.imshow(denoise_bilateral(X[0], multichannel=False), 'gray')\n",
    "plt.figure()\n",
    "plt.imshow(X[0], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41428/41428 [01:12<00:00, 570.84it/s]  | 46/41428 [00:00<01:31, 453.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.transform import rotate\n",
    "X_denoise_test = []\n",
    "\n",
    "for elem in tqdm(test_set):\n",
    "    #X_denoise_test.append(rotate(denoise_bilateral(elem,multichannel=False),\n",
    "                     #np.random.randint(low = -7, high=7) , resize=False))\n",
    "    X_denoise_test.append(denoise_bilateral(elem,multichannel=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_denoise_rotate', X_denoise_rotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n",
      "0 20000\n",
      "0 30000\n",
      "0 40000\n",
      "0 50000\n",
      "0 60000\n",
      "0 70000\n",
      "0 80000\n",
      "0 90000\n",
      "0 100000\n",
      "0 110000\n",
      "0 120000\n",
      "0 130000\n",
      "0 140000\n",
      "0 150000\n",
      "0 160000\n",
      "1 0\n",
      "1 10000\n",
      "1 20000\n",
      "1 30000\n",
      "1 40000\n",
      "1 50000\n",
      "1 60000\n",
      "1 70000\n",
      "1 80000\n",
      "1 90000\n",
      "1 100000\n",
      "1 110000\n",
      "1 120000\n",
      "1 130000\n",
      "1 140000\n",
      "1 150000\n",
      "1 160000\n"
     ]
    }
   ],
   "source": [
    "X_aug = []\n",
    "Y_aug = []\n",
    "\n",
    "for epoch in range(2):\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if i % 10000 == 0:\n",
    "            print(epoch, i)\n",
    "        temp = X[i]\n",
    "        temp = noise_filter(temp, disk(np.random.random()))\n",
    "\n",
    "        X_aug.append(rotate(X[i], np.random.randint(low = -30, high=30) , resize=False))\n",
    "        X_aug.append(rotate(temp, np.random.randint(low = -30, high=30) , resize=False))\n",
    "        Y_aug.append(Y[i])\n",
    "        Y_aug.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = int(len(X_aug)*7/10)\n",
    "np.save('X_aug', X_aug[:size])\n",
    "np.save('Y_aug', Y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_denoise_rotate = np.load(\"X_denoise_rotate.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_denoise_test = np.array(X_denoise_test)\n",
    "X_denoise_test.resize((len(X_denoise_test),40,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.005, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_aug, Y_aug[:int(len(Y_aug) * 7 / 10)], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_53 (Convolution2D) (None, 38, 38, 92)    920         convolution2d_input_35[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, 38, 38, 92)    0           convolution2d_53[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_41 (MaxPooling2D)   (None, 19, 19, 92)    0           activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_35 (BatchNorm (None, 19, 19, 92)    368         maxpooling2d_41[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_54 (Convolution2D) (None, 17, 17, 120)   99480       batchnormalization_35[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, 17, 17, 120)   0           convolution2d_54[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_42 (MaxPooling2D)   (None, 8, 8, 120)     0           activation_59[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_55 (Convolution2D) (None, 6, 6, 150)     162150      maxpooling2d_42[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, 6, 6, 150)     0           convolution2d_55[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_36 (BatchNorm (None, 6, 6, 150)     600         activation_60[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_56 (Convolution2D) (None, 4, 4, 180)     243180      batchnormalization_36[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_61 (Activation)       (None, 4, 4, 180)     0           convolution2d_56[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_43 (MaxPooling2D)   (None, 2, 2, 180)     0           activation_61[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 720)           0           maxpooling2d_43[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 500)           360500      flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_62 (Activation)       (None, 500)           0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_37 (BatchNorm (None, 500)           2000        activation_62[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 500)           250500      batchnormalization_37[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_63 (Activation)       (None, 500)           0           dense_20[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,119,698\n",
      "Trainable params: 1,118,214\n",
      "Non-trainable params: 1,484\n",
      "____________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:529 in _set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:536 in _set_model.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "Epoch 1/10\n",
      "118s - loss: 2.5436 - acc: 0.4449 - val_loss: 9.3493 - val_acc: 0.0060\n",
      "Epoch 2/10\n",
      "116s - loss: 0.8983 - acc: 0.7578 - val_loss: 0.5613 - val_acc: 0.8441\n",
      "Epoch 3/10\n",
      "115s - loss: 0.6846 - acc: 0.8085 - val_loss: 0.3532 - val_acc: 0.8849\n",
      "Epoch 4/10\n",
      "115s - loss: 0.5860 - acc: 0.8338 - val_loss: 0.6529 - val_acc: 0.8034\n",
      "Epoch 5/10\n",
      "115s - loss: 0.5269 - acc: 0.8496 - val_loss: 0.3989 - val_acc: 0.8873\n",
      "Epoch 6/10\n",
      "115s - loss: 0.4866 - acc: 0.8601 - val_loss: 0.3217 - val_acc: 0.8945\n",
      "Epoch 7/10\n",
      "115s - loss: 0.4577 - acc: 0.8679 - val_loss: 0.5187 - val_acc: 0.8537\n",
      "Epoch 8/10\n",
      "115s - loss: 0.4335 - acc: 0.8746 - val_loss: 0.5418 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "276s - loss: 0.4129 - acc: 0.8804 - val_loss: 0.2439 - val_acc: 0.9293\n",
      "Epoch 10/10\n",
      "374s - loss: 0.3954 - acc: 0.8853 - val_loss: 0.5877 - val_acc: 0.8465\n",
      "Test score: 0.587706813709\n",
      "Test accuracy: 0.846522781775\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "batch_size = 512\n",
    "nb_classes = 500\n",
    "nb_epoch = 10\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "input_shape = (40,40,1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Convolution2D(92, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(120, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(150, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(180, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorbord = TensorBoard(log_dir='/home/axcel/logs', histogram_freq=0,\n",
    "                                         write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=400),\n",
    "                    samples_per_epoch=len(X_train), nb_epoch=nb_epoch, verbose=2,\n",
    "                    validation_data=(X_test, Y_test), callbacks=[tensorbord] )\n",
    "\n",
    "#model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          #verbose=2, validation_data=(X_test, Y_test), callbacks=[tensorbord])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_aug = np.array(X_aug[:size], copy=False)\n",
    "X_aug.resize((len(X_aug),40,40,1))\n",
    "\n",
    "labels = np.unique(Y_aug)\n",
    "labels = labels.tolist()\n",
    "\n",
    "for i in range(len(Y_aug)):\n",
    "    Y_aug[i] = labels.index(Y_aug[i])\n",
    "\n",
    "X_aug = X_aug.astype(np.float16)\n",
    "Y_aug = np.array(Y_aug).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165874 samples, validate on 834 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:529 in _set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:536 in _set_model.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "Epoch 1/2\n",
      "116s - loss: 0.0392 - acc: 0.9892 - val_loss: 0.1128 - val_acc: 0.9580\n",
      "Epoch 2/2\n",
      "117s - loss: 0.0116 - acc: 0.9986 - val_loss: 0.0955 - val_acc: 0.9664\n",
      "Test score: 0.587706813709\n",
      "Test accuracy: 0.846522781775\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "batch_size = 400\n",
    "nb_classes = 500\n",
    "nb_epoch = 10\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "input_shape = (40,40,1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "tensorbord = TensorBoard(log_dir='/home/axcel/logs', histogram_freq=0,\n",
    "                                         write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "#model.fit_generator(datagen.flow(X_train, Y_train, batch_size=400),\n",
    "                    #samples_per_epoch=len(X_train), nb_epoch=5, verbose=2,\n",
    "                    #validation_data=(X_test, Y_test), callbacks=[tensorbord] )\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=2,\n",
    "          verbose=2, validation_data=(X_test, Y_test), callbacks=[tensorbord])\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"4 conv with maxpool and batchnorm 2 dense sigmoid on augmented 50 epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model(\"./Models Architecture/mnist_cnn_architecture_first_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n"
     ]
    }
   ],
   "source": [
    "import skimage.transform\n",
    "\n",
    "test_set = []\n",
    "i = 0\n",
    "for elem in test:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    test_set.append(skimage.transform.resize(elem, output_shape=(40,40)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = np.array(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41428, 40, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set.resize((41428,40,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = model.predict_classes(test_set, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch\")\n",
    "model2 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch + 10 augmented\")\n",
    "model3 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm\")\n",
    "model4 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm V2\")\n",
    "model5 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm V2 200 epoch\")\n",
    "model6 = load_model(\"./Models Architecture/mnist_cnn_architecture_first_sub\")\n",
    "model7 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch + 30 denoised and rotated\")\n",
    "model8 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm mode1 with 2 dense 50 epoch\")\n",
    "model9 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm 2 dense on augmented 50 epoch\")\n",
    "model10 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm 2 dense_small on augmented 50 epoch\")\n",
    "model11 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm 2 dense on augmented 40 epoch\")\n",
    "model12 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm 2 dense sigmoid on augmented 50 epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction1 = model1.predict_proba(test_set, verbose=2)\n",
    "prediction2 = model2.predict_proba(test_set, verbose=2)\n",
    "prediction3 = model3.predict_proba(test_set, verbose=2)\n",
    "prediction4 = model4.predict_proba(test_set, verbose=2)\n",
    "prediction5 = model5.predict_proba(test_set, verbose=2)\n",
    "prediction6 = model6.predict_proba(test_set, verbose=2)\n",
    "prediction7 = model7.predict_proba(test_set, verbose=2)\n",
    "prediction8 = model8.predict_proba(test_set, verbose=2)\n",
    "prediction9 = model9.predict_proba(test_set, verbose=2)\n",
    "prediction10 = model10.predict_proba(test_set, verbose=2)\n",
    "prediction11 = model11.predict_proba(test_set, verbose=2)\n",
    "prediction12 = model12.predict_proba(test_set, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summury_predict = (prediction1 + prediction2 + prediction3 +\n",
    "                   prediction4 + prediction5 + prediction6 +\n",
    "                   prediction7 + prediction8 + prediction9 +\n",
    "                   prediction10 + prediction11 + prediction12) / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = []\n",
    "for elem in summury_predict:\n",
    "    ans.append(np.argmax(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(ans)):\n",
    "    ans[i] = labels[ans[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [i+1 for i in range(len(test_set))]\n",
    "\n",
    "temp = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.concat([temp, ans], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.columns = ['Id', 'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = result.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt(\"ensamble12networkwith4augmented.csv\", result, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
