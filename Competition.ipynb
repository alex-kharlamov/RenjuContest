{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f90fe4fec695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.npy'"
     ]
    }
   ],
   "source": [
    "data = np.load(\"train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "for elem in data:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    X.append(tf.resize(elem[0], output_shape=(40,40)))\n",
    "    Y.append(elem[1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Y', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.load(\"X.npy\")\n",
    "Y = np.load(\"Y.npy\")\n",
    "\n",
    "X.resize((166708,40,40,1))\n",
    "\n",
    "labels = np.unique(Y)\n",
    "labels = labels.tolist()\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    Y[i] = labels.index(Y[i])\n",
    "\n",
    "X = X.astype(np.float16)\n",
    "Y = Y.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.load(\"X.npy\")\n",
    "Y = np.load(\"Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d1259c42ec6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskeletonize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestoration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdenoise_bilateral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenoise_bilateral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultichannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_adaptive\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.restoration import denoise_bilateral\n",
    "plt.imshow(denoise_bilateral(X[0], multichannel=False), 'gray')\n",
    "plt.figure()\n",
    "plt.imshow(X[0], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41428/41428 [01:12<00:00, 570.84it/s]  | 46/41428 [00:00<01:31, 453.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.transform import rotate\n",
    "X_denoise_test = []\n",
    "\n",
    "for elem in tqdm(test_set):\n",
    "    #X_denoise_test.append(rotate(denoise_bilateral(elem,multichannel=False),\n",
    "                     #np.random.randint(low = -7, high=7) , resize=False))\n",
    "    X_denoise_test.append(denoise_bilateral(elem,multichannel=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_denoise_rotate', X_denoise_rotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n",
      "0 20000\n",
      "0 30000\n",
      "0 40000\n",
      "0 50000\n",
      "0 60000\n",
      "0 70000\n",
      "0 80000\n",
      "0 90000\n",
      "0 100000\n",
      "0 110000\n",
      "0 120000\n",
      "0 130000\n",
      "0 140000\n",
      "0 150000\n",
      "0 160000\n",
      "1 0\n",
      "1 10000\n",
      "1 20000\n",
      "1 30000\n",
      "1 40000\n",
      "1 50000\n",
      "1 60000\n",
      "1 70000\n",
      "1 80000\n",
      "1 90000\n",
      "1 100000\n",
      "1 110000\n",
      "1 120000\n",
      "1 130000\n",
      "1 140000\n",
      "1 150000\n",
      "1 160000\n"
     ]
    }
   ],
   "source": [
    "X_aug = []\n",
    "Y_aug = []\n",
    "\n",
    "for epoch in range(2):\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if i % 10000 == 0:\n",
    "            print(epoch, i)\n",
    "        temp = X[i]\n",
    "        temp = noise_filter(temp, disk(np.random.random()))\n",
    "\n",
    "        X_aug.append(rotate(X[i], np.random.randint(low = -30, high=30) , resize=False))\n",
    "        X_aug.append(rotate(temp, np.random.randint(low = -30, high=30) , resize=False))\n",
    "        Y_aug.append(Y[i])\n",
    "        Y_aug.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = int(len(X_aug)*7/10)\n",
    "np.save('X_aug', X_aug[:size])\n",
    "np.save('Y_aug', Y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_denoise_rotate = np.load(\"X_denoise_rotate.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_denoise_test = np.array(X_denoise_test)\n",
    "X_denoise_test.resize((len(X_denoise_test),40,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_aug, Y_aug[:int(len(Y_aug) * 7 / 10)], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_flat = []\n",
    "\n",
    "for elem in X:\n",
    "    X_flat.append(elem.flatten())\n",
    "X_flat = np.array(X_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 13.3min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, verbose=1, max_depth=15, n_estimators=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_11 (Convolution2D) (None, 38, 38, 48)    480         convolution2d_input_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "prelu_10 (PReLU)                 (None, 38, 38, 48)    69312       convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 19, 19, 48)    0           prelu_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 19, 19, 48)    96          maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 17, 17, 64)    27712       batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "prelu_11 (PReLU)                 (None, 17, 17, 64)    18496       convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 8, 8, 64)      0           prelu_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 8, 8, 64)      128         maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 6, 6, 128)     73856       batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "prelu_12 (PReLU)                 (None, 6, 6, 128)     4608        convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 3, 3, 128)     0           prelu_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (None, 3, 3, 128)     256         maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1152)          0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 650)           749450      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "prelu_13 (PReLU)                 (None, 650)           650         dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_12 (BatchNorm (None, 650)           1300        prelu_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 550)           358050      batchnormalization_12[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "prelu_14 (PReLU)                 (None, 550)           550         dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_13 (BatchNorm (None, 550)           1100        prelu_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 500)           275500      batchnormalization_13[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 500)           0           dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1581544\n",
      "____________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:517 in _set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:521 in _set_model.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "Epoch 1/1\n",
      "84s - loss: 2.5946 - acc: 0.4466 - val_loss: 1.2201 - val_acc: 0.6877\n",
      "Test score: 1.22005164946\n",
      "Test accuracy: 0.687687447553\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "batch_size = 512\n",
    "nb_classes = 500\n",
    "nb_epoch = 10\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "input_shape = (40,40,1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "prelu = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Convolution2D(48, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "\n",
    "model.add(PReLU())\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64, kernel_size[0], kernel_size[1]))\n",
    "model.add(PReLU())\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(128, kernel_size[0], kernel_size[1]))\n",
    "model.add(PReLU())\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(650))\n",
    "model.add(PReLU())\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(550))\n",
    "model.add(PReLU())\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorbord = TensorBoard(log_dir='/home/axcel/logs', histogram_freq=0,\n",
    "                                         write_graph=True, write_images=False)\n",
    "\n",
    "model_saver = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.6f}.hdf5\",\n",
    "                                              monitor='val_acc', verbose=0,\n",
    "                                              save_best_only=True, save_weights_only=False,\n",
    "                                              mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=256),\n",
    "                    samples_per_epoch=len(X_train), nb_epoch=1, verbose=2,\n",
    "                    validation_data=(X_test, Y_test), callbacks=[tensorbord, model_saver])\n",
    "\n",
    "#model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          #verbose=2, validation_data=(X_test, Y_test), callbacks=[tensorbord])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_aug = np.array(X_aug[:size], copy=False)\n",
    "X_aug.resize((len(X_aug),40,40,1))\n",
    "\n",
    "labels = np.unique(Y_aug)\n",
    "labels = labels.tolist()\n",
    "\n",
    "for i in range(len(Y_aug)):\n",
    "    Y_aug[i] = labels.index(Y_aug[i])\n",
    "\n",
    "X_aug = X_aug.astype(np.float16)\n",
    "Y_aug = np.array(Y_aug).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model(\"./Models Architecture/4convpoolbigdense.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:517 in _set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/callbacks.py:521 in _set_model.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "Epoch 1/280\n",
      "80s - loss: 0.2146 - acc: 0.9329 - val_loss: 0.5315 - val_acc: 0.8654\n",
      "Epoch 2/280\n",
      "79s - loss: 0.2108 - acc: 0.9349 - val_loss: 0.3427 - val_acc: 0.9081\n",
      "Epoch 3/280\n",
      "79s - loss: 0.2045 - acc: 0.9365 - val_loss: 0.1726 - val_acc: 0.9517\n",
      "Epoch 4/280\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "batch_size = 400\n",
    "nb_classes = 500\n",
    "nb_epoch = 10\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "input_shape = (40,40,1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "tensorbord = TensorBoard(log_dir='/home/axcel/logs', histogram_freq=0,\n",
    "                                         write_graph=True, write_images=True)\n",
    "\n",
    "model_saver = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.6f}.hdf5\",\n",
    "                                              monitor='val_acc', verbose=0,\n",
    "                                              save_best_only=True, save_weights_only=False,\n",
    "                                              mode='auto')\n",
    "\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=400),\n",
    "                    samples_per_epoch=len(X_train), nb_epoch=280, verbose=2,\n",
    "                    validation_data=(X_test, Y_test), callbacks=[tensorbord, model_saver] )\n",
    "#model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=2,\n",
    "          #verbose=2, validation_data=(X_test, Y_test), callbacks=[tensorbord])\n",
    "#print('Test score:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"4 conv with maxpool and batchnorm 2 dense sigmoid on augmented 50 epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model(\"./Models Architecture/mnist_cnn_architecture_first_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n"
     ]
    }
   ],
   "source": [
    "import skimage.transform\n",
    "\n",
    "test_set = []\n",
    "i = 0\n",
    "for elem in test:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    test_set.append(skimage.transform.resize(elem, output_shape=(40,40)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = np.array(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set.resize((41428,40,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = model.predict_classes(test_set, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch\")\n",
    "model2 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch + 10 augmented\")\n",
    "model3 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm\")\n",
    "model4 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm V2\")\n",
    "model5 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm V2 200 epoch\")\n",
    "model6 = load_model(\"./Models Architecture/mnist_cnn_architecture_first_sub\")\n",
    "model7 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm with 2 dense 20 epoch + 30 denoised and rotated\")\n",
    "model8 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm mode1 with 2 dense 50 epoch\")\n",
    "model9 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm 2 dense on augmented 50 epoch\")\n",
    "model10 = load_model(\"./Models Architecture/3 conv with maxpool and batchnorm 2 dense_small on augmented 50 epoch\")\n",
    "model11 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm 2 dense on augmented 40 epoch\")\n",
    "model12 = load_model(\"./Models Architecture/4 conv with maxpool and batchnorm 2 dense sigmoid on augmented 50 epoch\")\n",
    "model13 = load_model(\"./Models Architecture/conv5.hdf5\")\n",
    "model14 = load_model(\"./Models Architecture/4convpoolbigdense.hdf5\")\n",
    "model15 = load_model(\"./Models Architecture/4convbatchmediumdense.hdf5\")\n",
    "model16 = load_model(\"./Models Architecture/3convprelu.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction1 = model1.predict_proba(test_set, verbose=2)\n",
    "prediction2 = model2.predict_proba(test_set, verbose=2)\n",
    "prediction3 = model3.predict_proba(test_set, verbose=2)\n",
    "prediction4 = model4.predict_proba(test_set, verbose=2)\n",
    "prediction5 = model5.predict_proba(test_set, verbose=2)\n",
    "prediction6 = model6.predict_proba(test_set, verbose=2)\n",
    "prediction7 = model7.predict_proba(test_set, verbose=2)\n",
    "prediction8 = model8.predict_proba(test_set, verbose=2)\n",
    "prediction9 = model9.predict_proba(test_set, verbose=2)\n",
    "prediction10 = model10.predict_proba(test_set, verbose=2)\n",
    "prediction11 = model11.predict_proba(test_set, verbose=2)\n",
    "prediction12 = model12.predict_proba(test_set, verbose=2)\n",
    "prediction13 = model13.predict_proba(test_set, verbose=2)\n",
    "prediction14 = model14.predict_proba(test_set, verbose=2)\n",
    "prediction15 = model15.predict_proba(test_set, verbose=2)\n",
    "prediction16 = model16.predict_proba(test_set, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction1 = model1.predict_proba(X_test, verbose=2)\n",
    "prediction2 = model2.predict_proba(X_test, verbose=2)\n",
    "prediction3 = model3.predict_proba(X_test, verbose=2)\n",
    "prediction4 = model4.predict_proba(X_test, verbose=2)\n",
    "prediction5 = model5.predict_proba(X_test, verbose=2)\n",
    "prediction6 = model6.predict_proba(X_test, verbose=2)\n",
    "prediction7 = model7.predict_proba(X_test, verbose=2)\n",
    "prediction8 = model8.predict_proba(X_test, verbose=2)\n",
    "prediction9 = model9.predict_proba(X_test, verbose=2)\n",
    "prediction10 = model10.predict_proba(X_test, verbose=2)\n",
    "prediction11 = model11.predict_proba(X_test, verbose=2)\n",
    "prediction12 = model12.predict_proba(X_test, verbose=2)\n",
    "prediction13 = model13.predict_proba(X_test, verbose=2)\n",
    "prediction14 = model14.predict_proba(X_test, verbose=2)\n",
    "prediction15 = model15.predict_proba(X_test, verbose=2)\n",
    "prediction16 = model16.predict_proba(X_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summury_predict = (prediction1 + prediction2 + prediction3 +\n",
    "                   prediction4 + prediction5 + prediction6 +\n",
    "                   prediction7 + prediction8 + prediction9 +\n",
    "                   prediction10 + prediction11 + prediction12 +\n",
    "                  prediction13 + prediction14 + prediction15) / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summury_predict = (best_alph[0]*prediction5 + best_alph[1]*prediction6 + best_alph[2]*prediction7\n",
    "                 + best_alph[3]*prediction8 + best_alph[4]*prediction9 +\n",
    "                   best_alph[5]*prediction10 + best_alph[6]*prediction11 + best_alph[7]*prediction12 +\n",
    "                  best_alph[8]*prediction13 + best_alph[9]*prediction14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summury_predict = (prediction5 +\n",
    "                   prediction7 + prediction8 + prediction9 +\n",
    "                   prediction10 + prediction11 + prediction12 +\n",
    "                  prediction13 + prediction14 + prediction16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "alph_coef = [1, 0.7, 0.2]\n",
    "best_alph = []\n",
    "max_acc = 0\n",
    "\n",
    "\n",
    "for i1 in tqdm(alph_coef):\n",
    "    for i2 in alph_coef:\n",
    "        for i3 in alph_coef:\n",
    "            for i4 in alph_coef:\n",
    "                for i5 in alph_coef:\n",
    "                    for i6 in alph_coef:\n",
    "                        for i7 in tqdm(alph_coef):\n",
    "                            for i8 in alph_coef:\n",
    "                                for i9 in alph_coef:\n",
    "                                    for i10 in alph_coef:\n",
    "                                        summury_predict = (i1*prediction5 + i2*prediction7\n",
    "                                                           + i3*prediction8 + i4*prediction9 +\n",
    "                                                           i5*prediction10 + i6*prediction11 +\n",
    "                                                           i7*prediction12 + i8*prediction13 + i9*prediction14\n",
    "                                                          + i10*prediction16)\n",
    "                                        ans = []\n",
    "                                        for elem in summury_predict:\n",
    "                                            ans.append(np.argmax(elem))\n",
    "                                        accuracy = accuracy_score(y_test, ans)\n",
    "                                        if accuracy > max_acc:\n",
    "                                            max_acc = accuracy\n",
    "                                            best_alph = [i1,i2,i3,i4,i5,i6,i7,i8,i9,i10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = []\n",
    "for elem in summury_predict:\n",
    "    ans.append(np.argmax(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(ans)):\n",
    "    ans[i] = labels[ans[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [i+1 for i in range(len(test_set))]\n",
    "\n",
    "temp = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.concat([temp, ans], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.columns = ['Id', 'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = result.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "numpy.savetxt(\"11netlatestprelu.csv\", result, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
